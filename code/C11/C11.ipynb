{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<style>\n",
    "p {\n",
    "    line-height: 1.5em;\n",
    "    font-size: 20px;\n",
    "}\n",
    "</style>\n",
    "---\n",
    "<center>\n",
    "<h1>第十一课</h1>\n",
    "</center>\n",
    "\n",
    "## **主题**：案例分析：决策树的应用\n",
    "\n",
    "## 纲要\n",
    "### 1. 决策树的工作原理\n",
    "### 2. 决策树的生成过程\n",
    "### 3. 构造一个决策分类树和一个决策回归树\n",
    "---\n",
    "\n",
    "\n",
    "<center><h2>1. 决策树的工作原理</h2></center>\n",
    "\n",
    "<p>\n",
    "在现实生活中，我们会遇到各种选择，不论挑选水果，还是网上购物，都是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，会发现它实际上是一个树状图，这就涉及到决策树模型。\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "比如我们要出门打篮球，一般会根据“天气”、“温度”、“湿度”、“刮风”这几个条件来判断，最后得到结果：去还是不去。\n",
    "</p>\n",
    "\n",
    "<center><img src=\"../../image/playbasketball.png\" /></center>\n",
    "\n",
    "<p>\n",
    "建立决策树模型的过程，一般要经历两个阶段：<b>构造</b>和<b>剪枝</b>。\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<b>构造的阶段——就是选择什么属性作为节点的过程</b>。在构造过程中，会存在三种节点：\n",
    "</p>\n",
    "\n",
    "<p>1. 根节点：就是树的最顶端，最开始的那个节点。在上图中，“天气”就是一个根节点；</p>\n",
    "\n",
    "<p>2. 内部节点：就是树中间的那些节点，比如说“温度”、“湿度”、“刮风”；</p>\n",
    "\n",
    "<p>3. 叶节点：就是树最底部的节点，也就是决策结果。</p>\n",
    "\n",
    "<p>\n",
    "<b>剪枝的阶段</b>——就是剪掉决策树中不太必要的节点。目的是防止“过拟合”（Overfitting）现象的发生。引起过拟合的原因主要是训练集太过理想，而出现分析结果很“呆板”的情况，最终导致泛化能力较弱。泛化能力指的分类器是通过训练集抽象出来的分类能力，可以理解是举一反三的能力。\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "剪枝操作可分为预剪枝和后剪枝两种。其中，预剪枝是在构造决策树时进行，后剪枝是在生成决策树之后进行。\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "对于上面到底要不要去打篮球的问题，显然将哪个属性（天气、温度、湿度、刮风）作为根节点是个关键问题，在这里我们先介绍两个指标：<b>纯度</b>和<b>信息熵</b>。\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "决策树的构造过程理解成为寻找纯净划分的过程。数学上，用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "举个例子，假设有 3 个集合：</p>\n",
    "<p><b>集合 1</b>：6 次都去打篮球；</p>\n",
    "<p><b>集合 2</b>：4 次去打篮球，2 次不去打篮球；</p>\n",
    "<p><b>集合 3</b>：3 次去打篮球，3 次不去打篮球。</p>\n",
    "\n",
    "<p>按照纯度指标来说，集合 1> 集合 2> 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<B>信息熵</B>（entropy）表示的是信息的不确定度。在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式。它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。\n",
    "</P>\n",
    "\n",
    "<p>构造决策树的时候，一般会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。</p>\n",
    "\n",
    "<center><img src=\"../../image/decidetree.png\" /></center>\n",
    "\n",
    "<p>\n",
    "我们重点来看下CART算法。CART 算法，英文全称叫做 Classification And Regression Tree，中文叫做分类回归树。ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "\n",
    "在Python中程序包sklearn可用来构建CART分类树和CART回归树。下面针对sklearn自带的葡萄酒数据集构造一棵分类决策树，并对sklearn 自带的波士顿房价数据集构造一棵回归树进行房价预测。\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".. _wine_dataset:\n\nWine recognition dataset\n------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 178 (50 in each of three classes)\n    :Number of Attributes: 13 numeric, predictive attributes and the class\n    :Attribute Information:\n \t\t- Alcohol\n \t\t- Malic acid\n \t\t- Ash\n\t\t- Alcalinity of ash  \n \t\t- Magnesium\n\t\t- Total phenols\n \t\t- Flavanoids\n \t\t- Nonflavanoid phenols\n \t\t- Proanthocyanins\n\t\t- Color intensity\n \t\t- Hue\n \t\t- OD280/OD315 of diluted wines\n \t\t- Proline\n\n    - class:\n            - class_0\n            - class_1\n            - class_2\n\t\t\n    :Summary Statistics:\n    \n    ============================= ==== ===== ======= =====\n                                   Min   Max   Mean     SD\n    ============================= ==== ===== ======= =====\n    Alcohol:                      11.0  14.8    13.0   0.8\n    Malic Acid:                   0.74  5.80    2.34  1.12\n    Ash:                          1.36  3.23    2.36  0.27\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n    Magnesium:                    70.0 162.0    99.7  14.3\n    Total Phenols:                0.98  3.88    2.29  0.63\n    Flavanoids:                   0.34  5.08    2.03  1.00\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n    Proanthocyanins:              0.41  3.58    1.59  0.57\n    Colour Intensity:              1.3  13.0     5.1   2.3\n    Hue:                          0.48  1.71    0.96  0.23\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n    Proline:                       278  1680     746   315\n    ============================= ==== ===== ======= =====\n\n    :Missing Attribute Values: None\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThis is a copy of UCI ML Wine recognition datasets.\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n\nThe data is the results of a chemical analysis of wines grown in the same\nregion in Italy by three different cultivators. There are thirteen different\nmeasurements taken for different constituents found in the three types of\nwine.\n\nOriginal Owners: \n\nForina, M. et al, PARVUS - \nAn Extendible Package for Data Exploration, Classification and Correlation. \nInstitute of Pharmaceutical and Food Analysis and Technologies,\nVia Brigata Salerno, 16147 Genoa, Italy.\n\nCitation:\n\nLichman, M. (2013). UCI Machine Learning Repository\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\nSchool of Information and Computer Science. \n\n.. topic:: References\n\n  (1) S. Aeberhard, D. Coomans and O. de Vel, \n  Comparison of Classifiers in High Dimensional Settings, \n  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n  Mathematics and Statistics, James Cook University of North Queensland. \n  (Also submitted to Technometrics). \n\n  The data was used with many others for comparing various \n  classifiers. The classes are separable, though only RDA \n  has achieved 100% correct classification. \n  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n  (All results using the leave-one-out technique) \n\n  (2) S. Aeberhard, D. Coomans and O. de Vel, \n  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n  Mathematics and Statistics, James Cook University of North Queensland. \n  (Also submitted to Journal of Chemometrics).\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='crit', options=('gini', 'entropy'), value='gini'), Dropdown(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad72d94e67894eb0bbe1fabdac4cae0e"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_wine\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from IPython.display import display                               \n",
    "from ipywidgets import interactive\n",
    "# 加载葡萄酒数据集\n",
    "data = load_wine()\n",
    "# 获取特征矩阵\n",
    "X = data.data\n",
    "# 获取目标向量\n",
    "y = data.target\n",
    "# 获取分类标识\n",
    "labels = data.feature_names\n",
    "# 打印数据集描述\n",
    "print(data.DESCR)\n",
    "def plot_tree(crit, split, depth, min_split, min_leaf=0.2):\n",
    "    estimator = DecisionTreeClassifier(random_state = 0 \n",
    "      , criterion = crit\n",
    "      , splitter = split\n",
    "      , max_depth = depth\n",
    "      , min_samples_split=min_split\n",
    "      , min_samples_leaf=min_leaf)\n",
    "    estimator.fit(X, y)\n",
    "    graph = Source(tree.export_graphviz(estimator\n",
    "        , out_file=None\n",
    "        , feature_names=labels\n",
    "        , class_names=['0', '1', '2']\n",
    "        , filled = True))\n",
    "    \n",
    "    display(SVG(graph.pipe(format='svg')))\n",
    "    return estimator\n",
    "\n",
    "inter = interactive(plot_tree \n",
    "   , crit = [\"gini\", \"entropy\"]\n",
    "   , split = [\"best\", \"random\"]\n",
    "   , depth=[1,2,3,4]\n",
    "   , min_split=(0.1,1)\n",
    "   , min_leaf=(0.1,0.5))\n",
    "display(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n 'B' 'LSTAT']\n回归树二乘偏差均值: 17.01802395209581\n回归树绝对值偏差均值: 3.0652694610778437\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# encoding=utf-8\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# 准备数据集\n",
    "boston=load_boston()\n",
    "# 探索数据\n",
    "print(boston.feature_names)\n",
    "# 获取特征集和房价\n",
    "features = boston.data\n",
    "prices = boston.target\n",
    "# 随机抽取33%的数据作为测试集，其余为训练集\n",
    "train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)\n",
    "# 创建CART回归树\n",
    "dtr=DecisionTreeRegressor()\n",
    "# 拟合构造CART回归树\n",
    "dtr.fit(train_features, train_price)\n",
    "# 预测测试集中的房价\n",
    "predict_price = dtr.predict(test_features)\n",
    "# 测试集的结果评价\n",
    "print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))\n",
    "print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) "
   ]
  },
  {
   "source": [
    "<center><img src=\"../../image/CART.png\" ></cetner>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}