{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<style>\n",
    "* {\n",
    "    font-family: consolas,'Microsoft YaHei';\n",
    "}\n",
    "p {\n",
    "    line-height: 1.5em;\n",
    "    font-size: 20px;\n",
    "}\n",
    "</style>\n",
    "---\n",
    "<center>\n",
    "<h1>第九课</h1>\n",
    "</center>\n",
    "\n",
    "## **主题**：自然语言处理及常用分析库的介绍\n",
    "\n",
    "<p>\n",
    "上节课介绍了结构化数据的存储方案，所用到的数据都是结构化的，如二维表结构数据、键值结构数据以及网状结构数据，今天这次课主要了解一下非结构化的数据—文本型数据。\n",
    "\n",
    "本节课的内容主要是有关自然语言的处理，涉及面几块内容：\n",
    "</p>\n",
    "\n",
    "## 纲要\n",
    "### 1. 什么是自然语言处理\n",
    "### 2. 结巴中文分词入门介绍\n",
    "### 3. 案例分析——《三体Ⅱ黑暗森林》情感分析\n",
    "### 4. 案例分析——电商产品评论数据情感分析\n",
    "### 5. NLTK的安装与使用\n",
    "---\n",
    "\n",
    "<center><img src=\"../image/dinggroup.jpg\" /></center>\n",
    "<center><h2>1. 什么是自然语言处理</h2></center>\n",
    "\n",
    "\n",
    "<p>\n",
    "自然语言处理（Natural Language Processing，简称 NLP）是研究计算机处理人类语言的一门技术。随着深度学习在图像识别、语音识别领域的大放异彩，人们对深度学习在 NLP 的价值也寄予厚望。再加上 AlphaGo 的成功，人工智能的研究和应用变得炙手可热。自然语言处理作为人工智能领域的认知智能，成为目前大家关注的焦点。NLP 研究领域包括：\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<b>1. 句法语义分析</b>：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n",
    "</p>\n",
    "<p>\n",
    "<b>2. 信息抽取</b>：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n",
    "</p>\n",
    "<p>\n",
    "<b>1. 文本挖掘（或者文本数据挖掘）</b>：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n",
    "</p>\n",
    "<p>\n",
    "<b>3. 机器翻译</b>：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n",
    "</p>\n",
    "<p>\n",
    "<b>4. 信息检索</b>：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用 1，2，3 的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n",
    "</p>\n",
    "<p>\n",
    "<b>5. 问答系统</b>： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。\n",
    "</p>\n",
    "<p>\n",
    "<b>6. 对话系统</b>：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。\n",
    "</p>\n",
    "\n",
    "> ——微软亚洲研究院首席研究员周明博士\n",
    "\n",
    "<p>\n",
    "我们这次课重点关注简单的句法语义分析过程。\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "<center><h2>2. 结巴中文分词入门介绍</h2></center>\n",
    "\n",
    "结巴分词是[github](https://github.com/fxsjy/jieba)开源的中文分词项目。具有以下特点：\n",
    "\n",
    "   - <p>支持三种分词模式：</p>\n",
    "\n",
    "        - <p>精确模式，试图将句子最精确地切开，适合文本分析；</p>\n",
    "        - <p>全模式，把句子中所有的可以成词的词语都扫描出来，速度非常快，但是不能解决歧义；</p>\n",
    "        - <p>搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</p>\n",
    "        - <p>paddle模式，利用PaddlePaddle深度学习框架，训练序列标注（双向GRU）网络模型实现分词。</p>\n",
    "   - <p>支持繁体分词</p>\n",
    "   - <p>支持自定义词典</p>\n",
    "\n",
    "<p>同时，它使用了如下算法：</p>\n",
    "\n",
    "   - <p>基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)</p>\n",
    "   - <p>采用了动态规划查找最大概率路径，找出基于词频的最大切分组合</p>\n",
    "   - <p>对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法</p>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import snownlp\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import jieba.posseg as pseg\n",
    "import re\n",
    "import wordcloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "'''\n",
    "关键词提取\n",
    "    1. 词频统计、降序排序\n",
    "    2. 人工去停用词\n",
    "'''\n",
    "def freq_word(words, rank):\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    freq_word = []\n",
    "    for word, freq in word_freq.items():\n",
    "        freq_word.append((word, freq))\n",
    "    freq_word.sort(key=lambda x: x[1], reverse=True)\n",
    "    result = []\n",
    "    for word, freq in freq_word[: rank]:\n",
    "        result.append({word, freq})\n",
    "    return result\n",
    "\n",
    "stopwords = []\n",
    "with open(\"stop_words.txt\", encoding='utf-8', mode=\"r\") as file:\n",
    "    for word in file:\n",
    "        stopwords.append(word.strip())\n",
    "\n",
    "print(stopwords)\n",
    "with open(\"谭德塞：使用疫苗结束新冠疫情有了真正的希望.txt\", encoding='utf-8', mode=\"r\") as file:\n",
    "    raw = file.read().strip()\n",
    "    article = re.compile('|'.join(stopwords)).sub('', raw)\n",
    "    jieba.add_word('谭德塞')\n",
    "    words = jieba.cut(article, cut_all=False)\n",
    "    rank = int(input(u\"请输入要获取前几位的高频词: \"))\n",
    "    print(freq_word(words, rank))\n",
    "    jieba.analyse.set_stop_words('stop_words.txt')\n",
    "    tags = jieba.analyse.extract_tags(raw, topK=rank)\n",
    "    print(\",\".join(tags))\n",
    "\n",
    "\n",
    "with open(\"谭德塞：使用疫苗结束新冠疫情有了真正的希望.txt\", encoding='utf-8', mode=\"r\") as file:\n",
    "    text = file.read()\n",
    "    print('3. 关键词提取')\n",
    "    print('-'*40)\n",
    "    print(' TF-IDF')\n",
    "    print('-'*40)\n",
    "    for x, w in jieba.analyse.extract_tags(text, withWeight=True):\n",
    "        print('%s %s' % (x, w))\n",
    "\n",
    "    print('-'*40)\n",
    "    print(' TextRank')\n",
    "    print('-'*40)\n",
    "\n",
    "    for x, w in jieba.analyse.textrank(text, withWeight=True, allowPOS=('ns', 'n', 'vn', 'v')):\n",
    "        print('%s %s' % (x, w))\n",
    "\n",
    "\n",
    "'''\n",
    "jieba.cut主要有三种模式\n",
    "'''\n",
    "raw = '真是好久好久没来哈皮娜拉野生动物园了，记忆里还是小时候三四年级学校组织春游去的银河系'\n",
    "# 全模式cut_all=True\n",
    "text = jieba.lcut(raw, cut_all=True)\n",
    "print('全模式分词：{ %d}' % len(text))\n",
    "print(\"/\".join(text))\n",
    "print('='*30)\n",
    "\n",
    "# 精准模式cut_all=False，默认即是\n",
    "text = jieba.lcut(raw, cut_all=False)\n",
    "print('精准模式分词：{ %d}' % len(text))\n",
    "print(\"/\".join(text))\n",
    "print('='*30)\n",
    "\n",
    "# 搜索引擎模式  cut_for_search\n",
    "text = jieba.lcut_for_search(raw)\n",
    "print('搜索引擎分词：{ %d}' % len(text))\n",
    "print(\"/\".join(text))\n",
    "print('='*30)\n",
    "\n",
    "print('总词数{}'.format(len(jieba.lcut(raw))))\n",
    "text = jieba.analyse.extract_tags(raw)\n",
    "print('关键词提取:', \"/\".join(text))\n",
    "text = jieba.analyse.extract_tags(raw, topK=3)\n",
    "print('关键词 top 3:', \"/\".join(text))\n",
    "print('='*30)\n",
    "\n",
    "text = jieba.lcut(raw, cut_all=False)\n",
    "print('原始分词: ', \"/\".join(text))\n",
    "jieba.add_word('哈皮娜拉')\n",
    "text = jieba.lcut(raw, cut_all=False)\n",
    "print('add_word添加自定义词后: ', \"/\".join(text))\n",
    "jieba.suggest_freq('野生动物园', tune=True)\n",
    "text = jieba.lcut(raw, cut_all=False)\n",
    "print('suggest_freq修正词频后: ', \"/\".join(text))\n",
    "jieba.load_userdict('userdict.txt')\n",
    "text = jieba.lcut(raw, cut_all=False)\n",
    "print('load_userdict使用词典后:'+\"/\".join(text))\n",
    "\n",
    "'''\n",
    "词典格式：\n",
    "  一词一行: 词语 词频（可省略） 词性（可省略）——（空格分隔，顺序固定，UTF-8编码）\n",
    "'''\n",
    "\n",
    "# cut返回一个generator生成器，lcut返回一个list\n",
    "\n",
    "'''\n",
    "词性标注\n",
    "'''\n",
    "words = pseg.cut('我爱美丽的北京天安门')\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))\n",
    "\n",
    "result = jieba.tokenize(u'永和服装饰品有限公司')\n",
    "for tk in result:\n",
    "    print('word %s\\t start: %d \\t end:%d' % (tk[0], tk[1], tk[2]))\n",
    "\n",
    "'''\n",
    "自定义语料库\n",
    "'''\n",
    "# jieba.analyse.set_idf_path(file_name)\n",
    "\n",
    "'''\n",
    "自定义分词器\n",
    "'''\n",
    "# jieba.posseg.POSTokenizer(tokenizer=None)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "<center><h2>3. 案例分析——《三体Ⅱ黑暗森林》情感分析</h2></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snownlp\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import jieba.posseg as pseg\n",
    "import re\n",
    "import wordcloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mk = np.array(Image.open('../image/china.jpg'))\n",
    "\n",
    "# 构建并配置两个词云对象w1和w2，分别存放积极词和消极词\n",
    "w1 = wordcloud.WordCloud(width=820,\n",
    "                         height=820,\n",
    "                         background_color='white',\n",
    "                         font_path='SourceHanSansSC-Bold.otf',\n",
    "                         mask=mk,\n",
    "                         scale=15)\n",
    "w2 = wordcloud.WordCloud(width=820,\n",
    "                         height=820,\n",
    "                         background_color='white',\n",
    "                         font_path='SourceHanSansSC-Bold.otf',\n",
    "                         mask=mk,\n",
    "                         scale=15)\n",
    "\n",
    "# 对来自外部文件的文本进行中文分词，得到积极词汇和消极词汇的两个列表\n",
    "f = open('三体黑暗森林.txt', encoding='utf-8')\n",
    "txt = f.read()\n",
    "txtlist = jieba.lcut(txt)\n",
    "positivelist = []\n",
    "negativelist = []\n",
    "\n",
    "# 下面对文本中的每个词进行情感分析，情感>0.96判为积极词，情感<0.06判为消极词\n",
    "print('开始进行情感分析，请稍等，大概需要两分钟左右')\n",
    "# 导入自然语言处理第三方库snownlp\n",
    "for each in txtlist:\n",
    "    each_word = snownlp.SnowNLP(each)\n",
    "    feeling = each_word.sentiments\n",
    "    if feeling > 0.96:\n",
    "        positivelist.append(each)\n",
    "    elif feeling < 0.06:\n",
    "        negativelist.append(each)\n",
    "    else:\n",
    "        pass\n",
    "# 将积极和消极的两个列表各自合并成积极字符串和消极字符串，字符串中的词用空格分隔\n",
    "positive_string = \" \".join(positivelist)\n",
    "negative_string = \" \".join(negativelist)\n",
    "\n",
    "# 将string变量传入w的generate()方法，给词云输入文字\n",
    "w1.generate(positive_string)\n",
    "w2.generate(negative_string)\n",
    "\n",
    "# 将积极、消极的两个词云图片导出到当前文件夹\n",
    "w1.to_file('positive.png')\n",
    "w2.to_file('negative.png')\n",
    "plt.rcParams['font.family'] = 'Microsoft YaHei'\n",
    "plt.rcParams['axes.facecolor'] = '#E5E5E5'\n",
    "plt.subplot(121)\n",
    "plt.imshow(w1, interpolation=\"bilinear\")\n",
    "plt.title('积极词汇')\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(w2, interpolation=\"bilinear\")\n",
    "plt.title('消极词汇')\n",
    "plt.axis('off')\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "<center><h2>4. 案例分析——电商产品评论数据情感分析</h2></center>\n",
    "\n",
    "<p>\n",
    "由于每个阶段的数据文件存在依赖关系，所以这里输出保存在了data/目录下。下面是该案例分析的五个过程，它们分别对应了代码实现中的五个函数：\n",
    "</p>\n",
    "\n",
    "- <p>\n",
    "programmer_1-->提取数据\n",
    "</p>\n",
    "- <p>\n",
    "programmer_2-->数据去重\n",
    "</p>\n",
    "- <p>\n",
    "programmer_3-->利用正则去除一些数据\n",
    "</p>\n",
    "- <p>\n",
    "programmer_4-->使用jieba分词\n",
    "</p>\n",
    "- <p>\n",
    "programmer_5-->分词之后的语义分析，LDA模型分析正面负面情感\n",
    "</p>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "删除了2725条评论\n",
      "0.047*\"安装\" + 0.024*\"买\" + 0.016*\"师傅\" + 0.014*\"热水器\" + 0.009*\"一个\" + 0.008*\"美的\" + 0.008*\"京东\" + 0.008*\"不错\" + 0.008*\"问题\" + 0.006*\"好\"\n",
      "0.046*\"安装\" + 0.030*\"不错\" + 0.013*\"热水器\" + 0.013*\"加热\" + 0.011*\"东西\" + 0.010*\"买\" + 0.010*\"使用\" + 0.010*\"有点\" + 0.008*\"好\" + 0.008*\"知道\"\n",
      "0.026*\"好\" + 0.023*\"安装\" + 0.023*\"热水器\" + 0.014*\"买\" + 0.013*\"美的\" + 0.012*\"知道\" + 0.008*\"加热\" + 0.008*\"安装费\" + 0.008*\"不错\" + 0.007*\"送货\"\n",
      "0.127*\"不错\" + 0.046*\"好\" + 0.031*\"东西\" + 0.024*\"使用\" + 0.019*\"感觉\" + 0.017*\"买\" + 0.016*\"值得\" + 0.015*\"高\" + 0.015*\"性价比\" + 0.013*\"热水器\"\n",
      "0.090*\"好\" + 0.062*\"安装\" + 0.032*\"送货\" + 0.026*\"不错\" + 0.025*\"很快\" + 0.023*\"速度\" + 0.023*\"加热\" + 0.023*\"师傅\" + 0.020*\"服务\" + 0.019*\"京东\"\n",
      "0.047*\"安装\" + 0.030*\"买\" + 0.016*\"美的\" + 0.013*\"热水器\" + 0.012*\"师傅\" + 0.011*\"一个\" + 0.009*\"好\" + 0.008*\"装\" + 0.008*\"安装费\" + 0.008*\"n\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "def programmer_1():\n",
    "    inputfile = \"data/huizong.csv\"\n",
    "    outputfile = \"data/meidi_jd.txt\"\n",
    "    data = pd.read_csv(inputfile, encoding=\"utf-8\")\n",
    "    data = data[[u\"评论\"]][data[u\"品牌\"] == u\"美的\"]\n",
    "    data.to_csv(outputfile, index=False, header=False, encoding=\"utf8\")\n",
    "\n",
    "\n",
    "def programmer_2():\n",
    "    inputfile = \"data/meidi_jd.txt\"\n",
    "    outputfile = \"data/meidi_jd_process_1.txt\"\n",
    "    data = pd.read_csv(inputfile, encoding=\"utf8\", header=None)\n",
    "    l1 = len(data)\n",
    "    data = pd.DataFrame(data[0].unique())\n",
    "    l2 = len(data)\n",
    "    data.to_csv(outputfile, index=False, header=False, encoding=\"utf8\")\n",
    "    print(u\"删除了%s条评论\" % (l1 - l2))\n",
    "\n",
    "\n",
    "def programmer_3():\n",
    "    inputfile1 = u\"data/meidi_jd_process_end_负面情感结果.txt\"\n",
    "    inputfile2 = u\"data/meidi_jd_process_end_正面情感结果.txt\"\n",
    "    outputfile1 = \"data/meidi_jd_neg.txt\"\n",
    "    outputfile2 = \"data/meidi_jd_pos.txt\"\n",
    "\n",
    "    data1 = pd.read_csv(inputfile1, encoding=\"utf8\", header=None)\n",
    "    data2 = pd.read_csv(inputfile2, encoding=\"utf8\", header=None)\n",
    "\n",
    "    data1 = pd.DataFrame(data1[0].str.replace(\".*?\\d+?\\\\t \", \"\"))\n",
    "    data2 = pd.DataFrame(data2[0].str.replace(\".*?\\d+?\\\\t \", \"\"))\n",
    "\n",
    "    data1.to_csv(outputfile1, index=False, header=False, encoding=\"utf8\")\n",
    "    data2.to_csv(outputfile2, index=False, header=False, encoding=\"utf8\")\n",
    "\n",
    "\n",
    "def programmer_4():\n",
    "\n",
    "    inputfile1 = \"data/meidi_jd_neg.txt\"\n",
    "    inputfile2 = \"data/meidi_jd_pos.txt\"\n",
    "    outputfile1 = \"data/meidi_jd_neg_cut.txt\"\n",
    "    outputfile2 = \"data/meidi_jd_pos_cut.txt\"\n",
    "\n",
    "    data1 = pd.read_csv(inputfile1, encoding=\"utf8\", header=None)\n",
    "    data2 = pd.read_csv(inputfile2, encoding=\"utf8\", header=None)\n",
    "\n",
    "    def mycut(s): return \" \".join(jieba.cut(s))\n",
    "\n",
    "    data1 = data1[0].apply(mycut)\n",
    "    data2 = data2[0].apply(mycut)\n",
    "\n",
    "    data1.to_csv(outputfile1, index=False, header=False, encoding=\"utf8\")\n",
    "    data2.to_csv(outputfile2, index=False, header=False, encoding=\"utf8\")\n",
    "\n",
    "\n",
    "def programmer_5():\n",
    "    negfile = \"data/meidi_jd_neg_cut.txt\"\n",
    "    posfile = \"data/meidi_jd_pos_cut.txt\"\n",
    "    stoplist = \"data/stoplist.txt\"\n",
    "\n",
    "    neg = pd.read_csv(negfile, encoding=\"utf8\", header=None)\n",
    "    pos = pd.read_csv(posfile, encoding=\"utf8\", header=None)\n",
    "    \"\"\"\n",
    "    sep设置分割词，由于csv默认半角逗号为分割词，而且该词恰好位于停用词表中\n",
    "    所以会导致读取错误\n",
    "    解决办法是手动设置一个不存在的分割词，这里使用的是tipdm\n",
    "    参数engine加上，指定引擎，避免警告\n",
    "    \"\"\"\n",
    "    stop = pd.read_csv(stoplist, encoding=\"utf8\",\n",
    "                       header=None, sep=\"tipdm\", engine=\"python\")\n",
    "\n",
    "    # pandas自动过滤了空格，这里手动添加\n",
    "    stop = [\" \", \"\"] + list(stop[0])\n",
    "\n",
    "    # 定义分割函数，然后用apply进行广播\n",
    "    neg[1] = neg[0].apply(lambda s: s.split(\" \"))\n",
    "    neg[2] = neg[1].apply(lambda x: [i for i in x if i not in stop])\n",
    "    pos[1] = pos[0].apply(lambda s: s.split(\" \"))\n",
    "    pos[2] = pos[1].apply(lambda x: [i for i in x if i not in stop])\n",
    "\n",
    "    # 负面主题分析\n",
    "    # 建立词典\n",
    "    neg_dict = corpora.Dictionary(neg[2])\n",
    "    # 建立语料库\n",
    "    neg_corpus = [neg_dict.doc2bow(i) for i in neg[2]]\n",
    "    # LDA模型训练\n",
    "    neg_lda = models.LdaModel(neg_corpus, num_topics=3, id2word=neg_dict)\n",
    "\n",
    "    for i in range(3):\n",
    "        print(neg_lda.print_topic(i))\n",
    "\n",
    "    # 正面主题分析\n",
    "    # 以下同上\n",
    "    pos_dict = corpora.Dictionary(pos[2])\n",
    "    pos_corpus = [pos_dict.doc2bow(i) for i in pos[2]]\n",
    "    pos_lda = models.LdaModel(pos_corpus, num_topics=3, id2word=pos_dict)\n",
    "    for i in range(3):\n",
    "        print(pos_lda.print_topic(i))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    programmer_1()\n",
    "    programmer_2()\n",
    "    programmer_3()\n",
    "    programmer_4()\n",
    "    programmer_5()\n",
    "    pass\n"
   ]
  },
  {
   "source": [
    "<center><h2>5. NLTK的安装与使用</h2></center>\n",
    "\n",
    "<p>NLTK 在使用 Python 处理自然语言的工具中处于领先的地位。它提供了超过 50 个包括如： WordNet 这种方便处理词汇资源的数据接口。同时，还有一套用于分类、标记化、词干标记、解析和语义推理的文本处理库，以及工业级 NLP 库的封装器。同时NLTK 是一个免费、开源的社区驱动的项目。</p>\n",
    "\n",
    "<p>在使用NLTK库前，我们先在命令行窗口输入如下命令安装NLTK及其语料分析库：</p>\n",
    "\n",
    "\n",
    "```bat\n",
    "pip install NLTK\n",
    "git clone https://github.com/nltk/nltk_data.git\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}